[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DAMCA Companion",
    "section": "",
    "text": "Introduction\nThis under construction site is the result of my engagement with Data Analysis: A Model Comparison Approach to Regression, ANOVA, and Beyond by Josh Correll, Abigail M. Folberg, Charles M. Judd, Gary H. McClelland, Carey S. Ryan.\nThe book is mostly great1 and I chose it recently as it “thinks” about models the way I have tried to teach students. I think it’s also pitched at a good level for sociology (etc.) graduate students who don’t have much stats background and for advanced social/behavioral science undergrads.\n1 I have a few beefs with it. The interpretation of odds ratios, the discussion of variable transformations via the ladder of powers, and no attention to contingency tables come to mind. But overall, I really like the book.I wanted to supplement the text myself for three main reasons:\n\nThe book itself doesn’t have any code and the accompanying instructor/student resources are written in base R. I wanted to translate things to tidyverse style to fit my own and my students’ preferred usage.\nThe book focuses on analytical approaches whereas I find it useful to demonstrate most concepts using simulation.\nI wanted a reason to fool around systematically with tinyplot. I’m usually a {ggplot2} person, but for some reason I’ve been obsessed with {tinyplot} lately. Am I eventually going to regret this unholy alliance of tidyverse and tinyplot? Maybe. But not today!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapter-01.html",
    "href": "chapter-01.html",
    "title": "Chapter 1",
    "section": "",
    "text": "Goals\nPractice some basic skills before getting into the content.",
    "crumbs": [
      "Chapter 1"
    ]
  },
  {
    "objectID": "chapter-01.html#set-up",
    "href": "chapter-01.html#set-up",
    "title": "Chapter 1",
    "section": "Set up",
    "text": "Set up\nLoad packages and set your graph theme.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(stringr)\nlibrary(tinyplot)\n\nlibrary(WDI)\n\ntinytheme(\"ipsum\",\n          family = \"Roboto Condensed\",\n          palette.qualitative = \"Tableau 10\",\n          palette.sequential = \"agSunset\")",
    "crumbs": [
      "Chapter 1"
    ]
  },
  {
    "objectID": "chapter-01.html#practice",
    "href": "chapter-01.html#practice",
    "title": "Chapter 1",
    "section": "Practice",
    "text": "Practice\nLet’s get (more or less) the same data the authors are using. You will need the {WDI} package from CRAN.1\n1 We will have a lot more countries using the WDI data directly.\nrawdata &lt;- WDI(indicator = \"IT.NET.USER.ZS\", \n               start = 2021, \n               end = 2021,\n               extra = TRUE)\n\nd &lt;- rawdata |&gt; \n  filter(region != \"Aggregates\") |&gt; \n  select(country, \n         iso = iso3c, \n         intpct = IT.NET.USER.ZS,\n         income) |&gt; \n  drop_na() |&gt; \n  mutate(myguess = 70,\n         residual = intpct - 70) # unconditional\n\nLet’s make different guesses for high-income and not high-income countries.\n\nd &lt;- d |&gt; \n  mutate(highinc = if_else(income == \"High income\", 1, 0),\n         my_cond_guess = if_else(highinc == 1, 90, 70),\n         my_cond_resid = intpct - my_cond_guess)\n\nsum(abs(d$residual))\n\n[1] 3681.513\n\nsum(abs(d$my_cond_resid))\n\n[1] 2804.545\n\n\nNotice that making separate guesses makes the ERROR (SSR or RSS or SSE) go down. That’s an improvement.2\n2 See the next chapter for what these terms mean in practice.Let’s make some visualizations.\nHere’s a histogram.\n\nplt(~ intpct,\n    data = d,\n    type = type_hist(),\n    main = \"Internet access by country, 2021\",\n    sub = \"World Development Indicators data\",\n    xlab = \"% households with internet\")\n\n\n\n\n\n\n\n\nHere’s a dotplot with the countries sorted by rank.\n\nplt(~ sort(intpct),\n    data = d,\n    main = \"Internet access by country, 2021\",\n    sub = \"World Development Indicators data\",\n    ylab = \"% households with internet\",\n    xaxt = \"n\",\n    xlab = \"\")",
    "crumbs": [
      "Chapter 1"
    ]
  },
  {
    "objectID": "chapter-02-03.html",
    "href": "chapter-02-03.html",
    "title": "Chapters 2 and 3",
    "section": "",
    "text": "Goals\nWe’ll go over some R code here to lock in the content from Chapter 2 (and a bit of 3).",
    "crumbs": [
      "Chapters 2 and 3"
    ]
  },
  {
    "objectID": "chapter-02-03.html#set-up",
    "href": "chapter-02-03.html#set-up",
    "title": "Chapters 2 and 3",
    "section": "Set up",
    "text": "Set up\nLoad packages here.\n\nlibrary(gssr)       # you may need to install\nlibrary(gssrdoc)    # ditto\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(stringr)\nlibrary(tinyplot)\n\ntinytheme(\"ipsum\",\n          family = \"Roboto Condensed\",\n          palette.qualitative = \"Tableau 10\",\n          palette.sequential = \"agSunset\")\n\nGet data from NORC one time and save it to your hard drive. I have the chunk option set to #| eval: false so this doesn’t run in the future.\n\ngss2024 &lt;- gss_get_yr(2024)\nsaveRDS(gss2024, file = here::here(\"data\", \"gss2024.rds\"))\n\nOnce it’s saved locally you can load it locally in the future.\n\ngss2024 &lt;- readRDS(file = here::here(\"data\", \"gss2024.rds\"))\n\nWe only need a small subset of variables to demonstrate today’s material so let’s get that.1\n1 This section uses haven::zap_labels() to get rid of the labels that come from the Stata import that is the origin of data in the {gssr} package. These can be useful sometimes but they can also get in the way of certain functions.\nd &lt;- gss2024 |&gt; \n  select(polviews, sex) |&gt; \n  drop_na() |&gt; \n  haven::zap_labels()",
    "crumbs": [
      "Chapters 2 and 3"
    ]
  },
  {
    "objectID": "chapter-02-03.html#unconditional-predictions-and-sse",
    "href": "chapter-02-03.html#unconditional-predictions-and-sse",
    "title": "Chapters 2 and 3",
    "section": "Unconditional predictions and SSE",
    "text": "Unconditional predictions and SSE\nWe will rely on the sum of squared errors (SSE) to quantify ERROR in this framework (at least for now). This is also sometimes called the sum of squared residuals (SSR) or the residual sum of squares (RSS). These are all the same quantities.\nWe can get R to estimate “model C” (our zero parameter guess).\n\nd &lt;- d |&gt; \n  mutate(mod_c_pred = 4,\n         mod_c_err = polviews - mod_c_pred)\n\nsse_c &lt;- sum(d$mod_c_err^2) # SSE for model C\nsse_c\n\n[1] 7408\n\n\nWe can now get R to estimate “model A” (our one parameter estimate).\n\nd &lt;- d |&gt; \n  mutate(mod_a_pred = mean(polviews),\n         mod_a_err = polviews - mod_a_pred)\n\nsse_a &lt;- sum(d$mod_a_err^2)\nsse_a\n\n[1] 7372.137\n\n\nWe can use these quantities to estimate PRE.\n\n(sse_c - sse_a) / sse_c\n\n[1] 0.004841087\n\n\nWe can also estimate model C with lm(). First let’s make a version of polviews “centered” on 4.\n\nd &lt;- d |&gt; \n  mutate(polviews_dev = polviews - 4)\n\nNow we can estimate model C by regressing polviews_dev in a model with no constant (i.e., estimating no parameters at all).\n\nmod_c &lt;- lm(polviews_dev ~ 0, \n            data = d)\ndeviance(mod_c) # confirm this is the same as above\n\n[1] 7408\n\n\nThis is similar for model A, except we now estimate the “intercept” by changing the formula to ~ 1. This asks R to estimate the intercept. Because of the way we transformed the outcome variable (deviating it from 4), this means “how different is the estimated mean from 4”.\n\nmod_a &lt;- lm(polviews_dev ~ 1,\n            data = d)\ndeviance(mod_a)\n\n[1] 7372.137\n\n(deviance(mod_c) - deviance(mod_a)) / deviance(mod_c)\n\n[1] 0.004841087\n\n\nThis makes a very small difference, reducing the SSE/SSR/RSS by .04%.\nIf we want, we can also use the predict() function after lm() to add the model estimate to the data frame. Here the same number will be added to each observation because the model is making the same guess for every row.\n\nd$mod_a_pred &lt;- predict(mod_a)\nd$mod_c_pred &lt;- predict(mod_c)\n\nglance() is also a useful function to look at models. It’s too early for us to dig into this too much, but sigma here (\\(\\sigma\\)) is the RMSE (the square root of the mean squared error).\n\nglance(mod_a)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1         0             0  1.53        NA      NA    NA -5806. 11616. 11629.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;",
    "crumbs": [
      "Chapters 2 and 3"
    ]
  },
  {
    "objectID": "chapter-02-03.html#conditional-predictions",
    "href": "chapter-02-03.html#conditional-predictions",
    "title": "Chapters 2 and 3",
    "section": "Conditional predictions",
    "text": "Conditional predictions\nWe won’t talk too much about this until the next chapter or two, but we can also use the data to make different predictions for different groups. For example, is the mean polviews different for male and female respondents?\n\nd2 &lt;- d |&gt; \n  group_by(sex) |&gt; \n  summarize(mean_pv = mean(polviews))\n\nd2\n\n# A tibble: 2 × 2\n    sex mean_pv\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     1    4.20\n2     2    4.03\n\n\nWe can also do this with lm() Let’s start with the “bad way” of conditional predictions. This is bad because it doesn’t make sense to have sex be coded 1 to 2.\n\nmodel3 &lt;- lm(polviews ~ 1 + sex,\n             data = d)\n\ntidy(model3)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    4.38     0.0890     49.3  0      \n2 sex           -0.178    0.0547     -3.26 0.00113\n\n\nOne good way is to make a new variable with a meaningful zero. That means that the intercept is now the prediction for the “zero group” or reference category.\n\nd &lt;- d |&gt; \n  mutate(male = if_else(sex == 1, 1, 0))\n\nmodel3 &lt;- lm(polviews ~ 1 + male,\n             data = d)\n\ntidy(model3)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    4.03     0.0368    109.   0      \n2 male           0.178    0.0547      3.26 0.00113\n\n\nAnother way is using the factor() wrapper to force R to make a dummy variable. Here it chose 1 (male) as the reference category and and 2 (female) as the “1” category. So the sign is different than when we manually chose above.\n\nmodel3 &lt;- lm(polviews ~ 1 + factor(sex),\n             data = d)\ntidy(model3)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     4.20     0.0405    104.   0      \n2 factor(sex)2   -0.178    0.0547     -3.26 0.00113\n\n\nIn any case, we can compare one-parameter models that make unconditional predictions (i.e., the same prediction for everyone) to two-parameter models that make predictions conditional on the sex of the respondent.\n\ndeviance(mod_a)  # just b0\n\n[1] 7372.137\n\ndeviance(model3) # b0 and b1\n\n[1] 7347.317\n\n(deviance(mod_a) - deviance(model3)) / deviance(mod_a)\n\n[1] 0.003366721\n\n\nHere the conditional model reduces the error by .34%. This isn’t a lot but how do we know if it’s “worth it”? We’ll talk about that later.",
    "crumbs": [
      "Chapters 2 and 3"
    ]
  },
  {
    "objectID": "chapter-04.html",
    "href": "chapter-04.html",
    "title": "Chapter 4",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Chapter 4"
    ]
  },
  {
    "objectID": "chapter-04.html#overview",
    "href": "chapter-04.html#overview",
    "title": "Chapter 4",
    "section": "",
    "text": "Goals\nWe’re going to look at sampling distributions and testing null hypotheses. So technically this starts with some content from Chapter 3.\n\n\nSet up\nLoad packages and set theme.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(tinyplot)\n\ntinytheme(\"ipsum\",\n          family = \"Roboto Condensed\",\n          palette.qualitative = \"Tableau 10\",\n          palette.sequential = \"agSunset\")",
    "crumbs": [
      "Chapter 4"
    ]
  },
  {
    "objectID": "chapter-04.html#sampling-distributions",
    "href": "chapter-04.html#sampling-distributions",
    "title": "Chapter 4",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nLoad saved GSS data.\n\ngss2024 &lt;- readRDS(file = here::here(\"data\", \"gss2024.rds\")) |&gt; \n  haven::zap_labels()\n\nGet some skewed data to show that we get a normal sampling distribution for the mean (with a big enough sample) no matter what the shape of the underlying distribution. We’ll use the GSS classic, tvhours.\n\nd &lt;- gss2024 |&gt; \n  select(tvhours) |&gt; \n  drop_na()\n\nplt(~ tvhours,\n    data = d,\n    type = type_hist(breaks = seq(-.5, 24.5, 1)),\n    main = \"Daily hours of television among US adults\",\n    sub = \"2024 General Social Survey\",\n    xaxt = \"n\",\n    xlim = c(-.6, 24.6))\naxis(1, at = seq(0, 24, 4), \n     labels = seq(0, 24, 4),\n     tck = 0,\n     lwd = 0)\n\n\n\n\n\n\n\n\nConsider this sample a population for now and take repeated samples from it. First step is to write a function that grabs a sample and computes the mean.\n\nget_sample_mean &lt;- function(n) {\n  \n  d |&gt; \n    slice_sample(n = n, replace = TRUE) |&gt; \n    summarize(m = mean(tvhours)) |&gt; \n    as.numeric()\n  \n}\n\nNow I like to make a simulation “skeleton” that I can plug results into.\n\nsims &lt;- tibble(\n  sim_number = 1:1000\n)\n\nNow I add the sampled means to the skeleton.\n\nsims &lt;- sims |&gt; \n  rowwise() |&gt; # do separately by row\n  mutate(m = get_sample_mean(n = 2152)) # vary the N\n\nNow I can plot the results.\n\nplt(~ m, \n    data = sims,\n    type = type_hist(freq = FALSE),\n    ylim = c(0, 6),\n    main = \"Simulated sampling distribution\",\n    sub = \"N = 2152\",\n    ylab = \"Density\",\n    xlab = \"Mean estimate\") \nplt_add(~ m,\n        data = sims,\n        type = type_density(bw = \"SJ\"),\n        col = \"black\")",
    "crumbs": [
      "Chapter 4"
    ]
  },
  {
    "objectID": "chapter-04.html#null-hypothesis",
    "href": "chapter-04.html#null-hypothesis",
    "title": "Chapter 4",
    "section": "Null hypothesis",
    "text": "Null hypothesis\n\nSetting up the null model\nNull hypothesis: the average American adult in 2024 watches 3 hours of TV per day. By subtracting 3 from the observed value, we can make it so the null hypothesis is 0. That makes it easy to use lm(y ~ 0) to set up the null model.\n\nd &lt;- d |&gt; \n  mutate(tvdev = tvhours - 3)\n\nm0 &lt;- lm(tvdev ~ 0, data = d)\nm1 &lt;- lm(tvdev ~ 1, data = d)\n\nsse0 &lt;- deviance(m0)\nsse1 &lt;- deviance(m1)\n\nobserved_pre &lt;- (sse0 - sse1) / sse0\nobserved_pre\n\n[1] 0.008343581\n\n\nThe SSE is improved by estimating \\(\\beta_0\\), but was it improved more than we’d expect by chance?\n\n\nCreating a null distribution\nHere’s a quick example where we make data where the null hypothesis is TRUE. Then we can use it to check whether we could get numbers that big by chance.\n\nfake_data &lt;- tibble(tvhours = rnorm(2152, mean = 3, sd = 3.3))\n\nfake_data &lt;- fake_data |&gt; \n  mutate(tvdev = tvhours - 3)\n\nm0 &lt;- lm(tvdev ~ 0, data = fake_data)\nm1 &lt;- lm(tvdev ~ 1, data = fake_data)\n\nsse0 &lt;- deviance(m0)\nsse1 &lt;- deviance(m1)\n\n(sse0 - sse1) / sse0\n\n[1] 0.002183917\n\n\nConvert this idea to a function so we can do this many times. The basic idea is to simulate data where the null is true, then calculate the PRE (which won’t be exactly zero because of sampling variability).1\n1 We could create a better null distribution here by using a different distribution than the normal for our fake tvhours data. As you saw earlier the data itself is actually NOT normally distributed. We could use a count distribution but we’re not ready for that! We’ll talk about that more later.\ncalc_null_pre &lt;- function() {\n  \n  null_data &lt;- tibble(tvdev = rnorm(2152, 0, 3.3))\n  \n  m0 &lt;- lm(tvdev ~ 0, data = null_data)\n  m1 &lt;- lm(tvdev ~ 1, data = null_data) \n  \n  pre &lt;- (deviance(m0) - deviance(m1)) / deviance(m0)\n  \n  return(pre) # this returns the observed PRE from that simulation\n  \n}\n\nCreate a skeleton and append 1000 null PRE simulations.\n\nnull_sims &lt;- tibble(sim_number = 1:1000) |&gt; \n  rowwise() |&gt; \n  mutate(pre = calc_null_pre())\n\n\n\nThe test\nThe idea is to compare the real world to the world implied by the null hypothesis. So we’ll show how the OBSERVED PRE compares to the distribution of NULL PREs.\n\nplt(~ pre,\n    data = null_sims,\n    type = \"hist\",\n    xlim = c(0, .009))\nplt_add(type = type_vline(v = observed_pre),\n        col = \"#E69F00\",\n        lwd = 2)\n\n\n\n\n\n\n\n\nBased on the formula in the book, what F would that be equivalent to?\n\nfstat &lt;- (observed_pre / 1) / ((1 - observed_pre) / (nrow(d) - 1))\nfstat\n\n[1] 18.09805\n\n\nF-distribution with numerator df = 1 and denominator df = 2151. The dotted line is the critical value and the solid line is the observed value (wayyyy above that).\n\n\nShow code\ndf1 &lt;- 1\ndf2 &lt;- 2151\nalpha &lt;- .01\nf_obs &lt;- 18.1\n\nf_crit &lt;- qf(1 - alpha, df1, df2)\nx_max  &lt;- 20\n\n# grid for the curve\nx &lt;- seq(0, x_max, length.out = 2000)\ny &lt;- df(x, df1 = df1, df2 = df2)\n\n# main curve\nplt(y ~ x,\n    type = \"l\",\n    main = \"F(1, 2151)\",\n    xlab = \"F\",\n    ylab = \"Density\",\n    xlim = c(0, x_max),\n    lwd = 2)\n\n# critical line + observed line\nabline(v = f_crit, lty = 2, lwd = 2)          # dashed\nabline(v = f_obs,  col = \"#E69F00\", lwd = 2)\n\n\n\n\n\n\n\n\n\nYou can get F* (the critical value) from the book. Or we can do it using functions in R (as we did above). Remember that there is nothing sacred about 95% or 99% or any of that.\n\nqf(.99, 1, 2151)\n\n[1] 6.646687\n\n\nYou can also get the p-value from functions rather than from the book.\n\n1 - pf(fstat, 1, 2151)\n\n[1] 2.188091e-05\n\n\nIn any case, we will be rejecting the null hypothesis here!",
    "crumbs": [
      "Chapter 4"
    ]
  },
  {
    "objectID": "chapter-05.html",
    "href": "chapter-05.html",
    "title": "Chapter 5",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Chapter 5"
    ]
  },
  {
    "objectID": "chapter-05.html#overview",
    "href": "chapter-05.html#overview",
    "title": "Chapter 5",
    "section": "",
    "text": "Goals\nIn this section we’re going to look at power, effect sizes, and confidence intervals.\n\n\nSet up\nLoad packages and set theme here.\n\nlibrary(gssr)\nlibrary(gssrdoc)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tinyplot)\n\ntinytheme(\"ipsum\",\n          family = \"Roboto Condensed\",\n          palette.qualitative = \"Tableau 10\",\n          palette.sequential = \"agSunset\")\n\nWe’re going to use the 2010 GSS panel data. This is how I got the data the first time using Kieran Healy’s {gssr} package. Then I saved it locally. I set #| eval: false in the code chunk below now so that it doesn’t run every time.\n\ndata(\"gss_panel10_long\")\nsaveRDS(gss_panel10_long, \n        file = here::here(\"data\", \"gss_panel10_long.rds\"))\n\nLoad the locally saved GSS data.\n\ngss_panel10_long &lt;- read_rds(here::here(\"data\", \"gss_panel10_long.rds\"))",
    "crumbs": [
      "Chapter 5"
    ]
  },
  {
    "objectID": "chapter-05.html#power",
    "href": "chapter-05.html#power",
    "title": "Chapter 5",
    "section": "Power",
    "text": "Power\n\nMotivating example: TV hours\nWe are going to look at studying how US adults might have changed the number of TV hours they watch. This first chunk is just getting some GSS data and calculating each respondent’s difference in reported hours from 2010 to 2014.\n\nd &lt;- gss_panel10_long |&gt; \n  select(firstid, wave, y = tvhours) |&gt; \n  filter(wave %in% c(1, 3)) |&gt; \n  drop_na() |&gt;\n  mutate(y = as.numeric(y)) |&gt; \n  mutate(num_obs = n(), .by = firstid) |&gt; \n  filter(num_obs == 2) |&gt; \n  select(-num_obs) |&gt; \n  pivot_wider(id_cols = firstid,\n              names_prefix = \"y\",\n              names_from = wave,\n              values_from = y) |&gt; \n  mutate(ydiff = y3 - y1)\n\nplt(~ ydiff,\n    data = d,\n    main = \"Difference in hours of TV watching, 2010-2014\",\n    sub = \"2010 General Social Survey Panel Data\",\n    type = type_hist(breaks = seq(-23.5, 23.5, 1)))\n\n\n\n\n\n\n\n\nSince we’re talking about a change, there’s a natural null hypothesis here, which is “no change.” In other words, that \\(E(Y_{i3} - Y_{i1}) = 0\\).\nThere are easier ways to do this but let’s keep going with the logic of the book. We’ll estimate a model where the mean difference is assumed to be zero (MC) and a model where we estimate the mean difference from data (MA).\n\nMC &lt;- lm(ydiff ~ 0, data = d)\nMA &lt;- lm(ydiff ~ 1, data = d)\n\nsse_MC &lt;- deviance(MC)\nsse_MA &lt;- deviance(MA)\n\n(sse_MC - sse_MA) / sse_MC\n\n[1] 0.0003702235\n\n\nThis is a teeeeeeeny tiny PRE. By checking mean(d$ydiff) we can see that the mean difference is -0.0455556. So the low PRE value isn’t a big surprise here.\nBut maybe the world has changed since 2014 and I want to check if people changed between 2026 and 2030? How many respondents would I need to recruit for my study?\n\n\nCalculating power\nAs the book explains, figuring out how many respondents you need to detect an effect is a question of power. Here’s the power calculation assuming the minimum interesting change would be 30 minutes and that the SD of the difference is the same observed in the GSS. We’ll use the power.t.test() function here but we will talk more about the t distribution (and test) a bit later. For now just go with it!\n\n# minimum interesting change (choice increase or decrease of 30 minutes)\ndelta &lt;- .5\n\n# estimate of variance of differences\nsd_diff &lt;- sd(d$ydiff)  # from GSS\n\n# plug everything into the formula\npower.t.test(delta = delta,\n             sd = sd_diff,\n             type = \"one.sample\", # I am comparing one sample's difference score\n             sig.level = .01,\n             power = .8)\n\n\n     One-sample t test power calculation \n\n              n = 265.3863\n          delta = 0.5\n             sd = 2.368483\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\n\nSo we’d need a sample of at least 266 people to have at least an 80% chance of rejecting the null hypothesis of no difference if in reality people will (on average) increase or decrease their TV watching by at least 30 minutes over the next 4 years. (This assumes we’ll use an alpha level of .01.)\nWe can look at this in a slightly different way by seeing what power we’d have for different sample sizes. We’ll make a reusable get_power() function, make a “skeleton” with lots of different sample sizes, compute for each, then visualize.\n\n# get power as a function of a given sample size, etc.\nget_power &lt;- function(n) {\n  tmp &lt;- power.t.test(delta = delta,\n                      sd = sd_diff,\n                      type = \"one.sample\",\n                      sig.level = .01,\n                      n = n)\n  return(tmp$power)\n}\n\n# make skeleton\npower_sims &lt;- tibble(n = seq(from = 10, \n                             to = 600, \n                             by = 10))\n\n# add power results to skeleton\npower_sims &lt;- power_sims |&gt; \n  rowwise() |&gt; \n  mutate(power = get_power(n))\n\n# plot the result\nplt(power ~ n,\n    data = power_sims,\n    type = \"l\",\n    lwd = 2,\n    xlab = \"Sample size\",\n    ylab = \"Power\",\n    main = \"Power by sample size\")\n\n\n\n\n\n\n\n\nGiven a minimum effect size, expected SD, and alpha level, power increases as a function of the sample size. With around 150 people, we could only detect the difference about half the time even if it’s there. And we definitely wouldn’t need more than 600 people, as the power is nearly 100%.\n\n\nSimulation power analysis\nThese calculators are great but we can learn more about power analysis by simulation. I want to write a function that does my whole study from start to finish (i.e., collects data, tests null hypothesis, reports result) with a particular sample size. The goal is to see how often I reject the null hypothesis given that it’s actually false. I will create my own hypothetical world with a known change (30 minutes) and SD (2.37; estimated from the GSS) and then conduct my experiment many, many times under different sample sizes and alpha levels so see how often I can (correctly) reject the null hypothesis.\nWhat this do_study() function returns is a 1 if the null hypothesis is rejected and a 0 if it can’t be rejected.\n\n# create \"do study function!\n\ndo_study &lt;- function(delta, sd, alpha, n) {\n  \n  # create fake data with minimal interesting change\n  d &lt;- tibble(\n    diff = rnorm(n, delta, sd)\n  )\n  \n  # estimate Model C and Model A\n  mc &lt;- lm(diff ~ 0, data = d)\n  ma &lt;- lm(diff ~ 1, data = d)\n  \n  # estimate observed PRE\n  sse_c &lt;- deviance(mc)\n  sse_a &lt;- deviance(ma)\n  \n  pre &lt;- (sse_c - sse_a) / sse_c\n  \n  df1 &lt;- length(coef(ma)) - length(coef(mc))\n  df2 &lt;- n - length(coef(ma))\n  \n  # calculate critical value of PRE\n  crit_f &lt;- qf(p = 1 - alpha,\n               df1 = df1,\n               df2 = df2)\n  \n  crit_pre &lt;- crit_f / (crit_f + df2/df1)\n  \n  # STUDY RESULT: return a 1 if reject, a 0 if fail to reject\n  return(as.numeric(pre &gt; crit_pre)) # this is a logical to 0/1\n  \n}\n\nNow we can test it out under different conditions and see what the rejection rate looks like.\n\n# create skeleton with variable values\n\nset.seed(0206)                   # for reproducibility\n\nsim_studies &lt;- \n  expand_grid(\n    sid = 1:500,                 # 500 times per combo\n    delta = .5,                  # minimum interesting change\n    sd = 2.37,                   # this is from observed GSS difference\n    n = seq(50, 600, 50),        # from 50 to 350 by 25s\n    alpha = c(.05, .005)         # different alpha levels\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(reject = do_study(      # \"do the studies\"\n    delta = delta,\n    sd = sd,\n    n = n,\n    alpha = alpha\n  ))\n\n# summarize results by condition\nsim_results &lt;- sim_studies |&gt; \n  group_by(n, alpha) |&gt; \n  summarize(prob_reject = mean(reject), # rejection proportions\n            .groups = \"drop\")           # don't need anymore\n\n# plot the results\nplt(prob_reject ~ n | factor(alpha), \n    data = sim_results, \n    type = \"b\",\n    legend = legend(\"topright!\",\n                    title = expression(alpha)),\n    xlab = \"N\",\n    ylab = \"Observed rejection rate\",\n    main = \"Power simulation results\",\n    sub = \".5 hour change; SD = 2.37\")\n\n\n\n\n\n\n\n\nThis is “messier” than a power calculator but you can see the same basic findings you’d expect. And it (hopefully) gives you a better sense of power as “the probability of rejecting the null hypothesis under a given set of conditions.”[^fn51]",
    "crumbs": [
      "Chapter 5"
    ]
  },
  {
    "objectID": "chapter-05.html#effect-sizes",
    "href": "chapter-05.html#effect-sizes",
    "title": "Chapter 5",
    "section": "Effect sizes",
    "text": "Effect sizes\nWe picked a 30-minute change as the minimum interesting effect size. But “how big” is this difference? As TV watchers, we have an intuitive sense. But a more general way of quantifying effect size is Cohen’s d. As outlined in the book, this is just the difference divided by the SD. So for the change in TV time it would be .5 hours / 2.37 hours = .21.\nFor a one-parameter comparison like we have here, Cohen’s d is related to PRE (or \\(\\eta^2\\) or \\(R^2\\)) as follows:\n\\[d = 2 \\sqrt{\\frac{\\eta^2}{1- \\eta^2}}\\]\nOr the other way around:\n\\[\\eta^2 = \\frac{d^2}{d^2 + 4}\\]\nSo the minimum PRE we were looking for in our power analysis would be given by .21^2 / (.21^2 + 4) or about .011.\nAlthough it’s common (people love guidelines!) you should NOT mechanically use Cohen’s d type values to interepret effect sizes in terms of small, medium, or large. But it’s useful to visualize what some differences look like.",
    "crumbs": [
      "Chapter 5"
    ]
  },
  {
    "objectID": "chapter-05.html#confidence-intervals",
    "href": "chapter-05.html#confidence-intervals",
    "title": "Chapter 5",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nCIs are based on the normal distribution (for large samples) or the t-distribution (for smaller samples).\n\nt distribution\nThe t distribution looks more and more like the normal as the sample size (df) goes up.\n\n\n\n\n\n\n\n\n\n\n\nCalculating a confidence interval\nTo get a confidence interval of an estimate (e.g., the mean number of hours people changed their TV watching), we need three ingredients:\n\nthe estimated mean\nthe estimated standard error (i.e., the estimated SD of the sampling distribution)\nthe theory-based width of the relevant distribution (e.g., normal or t)\n\nFor the mean weight of cars in mtcars, we can just do:\n\nm &lt;- mean(mtcars$wt)\nm\n\n[1] 3.21725\n\n\nFor the standard error of the mean, we calculate it as:\n\\[se = \\frac{s}{\\sqrt{n}}\\]\n\nse &lt;- sd(mtcars$wt) / sqrt(nrow(mtcars))\nse\n\n[1] 0.1729685\n\n\nNow we need the width of the t-distribution with 31 (\\(n-1\\)) degrees of freedom for a 99% confidence interval.\n\nwidth &lt;- qt(.995, 31)\nwidth\n\n[1] 2.744042\n\n\nNow we can put it together into an upper bound and a lower bound of a confidence interval.\n\nub &lt;- m + se*width\nlb &lt;- m - se*width\nc(lb, ub)\n\n[1] 2.742617 3.691883\n\n\n\n\nInterpreting a confidence interval\nThis is trickier than it seems. You want to be able to say something like “we’re 99% sure that the true value is in this interval.” But that’s not quite right. The 99% confidence is about the procedure, not about the estimate you get from real data. The technically correct definition is:\n\nIf you were to repeat the entire data-generating-and-interval-building process indefinitely under identical conditions (same model, same population mean), then 99% of the intervals produced by this method would contain the true mean and 1% would not.\n\n\n\nt and F\nFor a one-parameter comparison, the F-statistic is just the square of the t-statistic. To illustrate, let’s revisit our null hypothesis that people did NOT change their TV viewing hours between 2010 to 2014.\n\nmc &lt;- lm(ydiff ~ 0, data = d)\nma &lt;- lm(ydiff ~ 1, data = d)\n\nanova(mc, ma) \n\nAnalysis of Variance Table\n\nModel 1: ydiff ~ 0\nModel 2: ydiff ~ 1\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1    900 5045.0                          \n2    899 5043.1  1    1.8678 0.333 0.5641\n\ntidy(ma)      \n\n# A tibble: 1 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.0456    0.0789    -0.577   0.564\n\n\nThe anova() function is a shortcut for calculating the F-statistic between the models. The statistic value in the tidy() output shows the t-statistic or how many SEs away from the null is the observed value. As you can see, -.5772 = .333. The p-values are the same as well.",
    "crumbs": [
      "Chapter 5"
    ]
  },
  {
    "objectID": "section-a-recap.html",
    "href": "section-a-recap.html",
    "title": "Section A Recap",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Section A Recap"
    ]
  },
  {
    "objectID": "section-a-recap.html#overview",
    "href": "section-a-recap.html#overview",
    "title": "Section A Recap",
    "section": "",
    "text": "Goals\nI’m not going to talk about everything in this section here. But I do want to mention one thing that is going to be important: maximum likelihood.\n\n\nSession set up",
    "crumbs": [
      "Section A Recap"
    ]
  },
  {
    "objectID": "section-a-recap.html#maximum-likelihood",
    "href": "section-a-recap.html#maximum-likelihood",
    "title": "Section A Recap",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nSo far we have been interesting in minimizing the sum of squared residuals. This makes sense when we’re dealing with outcome whose residuals can be modeled as a normal distribution. But, as we’ll see later, that’s not always the case.\nIt turns out that minimizing the sum of squared residuals is equivalent to maximizing the likelihood of the observations. Which is a procedure we can use with basically every kind of model later on.\nThe relationship between the residual sum of squares (RSS) and the likelihood of the observations is:\n\\[\\ell\n= -\\frac{n}{2}\\left[1+\\log(2\\pi)+\\log\\!\\left(\\frac{\\mathrm{RSS}}{n}\\right)\\right]\\]\nYou do NOT have to memorize this. But it’s good to think about.\nHere’s a quick comparison in R for you to see the equivalence.\n\nfit &lt;- lm(wt ~ 1, data = mtcars)\ndeviance(fit)\n\n[1] 29.67875\n\nrss &lt;- deviance(fit)\nn &lt;- nobs(fit)\n\nll_from_rss &lt;- -(n/2) * (1 + log(2*pi) + log(rss/n))\nll_from_rss\n\n[1] -44.20116\n\nlogLik(fit) # same!\n\n'log Lik.' -44.20116 (df=2)\n\n\nIf you really want to understand likelihood, it’s valuable to think about it as a pointwise or rowwise quantity. That is, given a specific model, you can evaluate the likelihood of any individual observation.\n\nm_hat &lt;- fit$coefficients[1]\ns_hat &lt;- sd(fit$residuals)\n\nmtcars &lt;- mtcars |&gt; \n  mutate(likelihood = dnorm(wt, m_hat, s_hat),\n         log_lik = log(likelihood))\n\nmtcars |&gt; \n  arrange(wt) |&gt; \n  select(wt, likelihood, log_lik)\n\n                       wt likelihood    log_lik\nLotus Europa        1.513 0.08945265 -2.4140458\nHonda Civic         1.615 0.10668155 -2.2379071\nToyota Corolla      1.835 0.15031880 -1.8949969\nFiat X1-9           1.935 0.17276190 -1.7558409\nPorsche 914-2       2.140 0.22241163 -1.5032254\nFiat 128            2.200 0.23749870 -1.4375931\nDatsun 710          2.320 0.26777471 -1.3176093\nToyota Corona       2.465 0.30340200 -1.1926966\nMazda RX4           2.620 0.33842446 -1.0834544\nFerrari Dino        2.770 0.36728051 -1.0016294\nVolvo 142E          2.780 0.36898104 -0.9970100\nMazda RX4 Wag       2.875 0.38353077 -0.9583354\nMerc 230            3.150 0.40676384 -0.8995225\nFord Pantera L      3.170 0.40725061 -0.8983265\nMerc 240D           3.190 0.40756765 -0.8975484\nHornet 4 Drive      3.215 0.40772466 -0.8971632\nAMC Javelin         3.435 0.39775323 -0.9219235\nHornet Sportabout   3.440 0.39729596 -0.9230738\nMerc 280            3.440 0.39729596 -0.9230738\nMerc 280C           3.440 0.39729596 -0.9230738\nValiant             3.460 0.39536891 -0.9279360\nDodge Challenger    3.520 0.38866808 -0.9450296\nDuster 360          3.570 0.38207185 -0.9621466\nMaserati Bora       3.570 0.38207185 -0.9621466\nMerc 450SL          3.730 0.35541503 -1.0344691\nMerc 450SLC         3.780 0.34557225 -1.0625535\nCamaro Z28          3.840 0.33297034 -1.0997018\nPontiac Firebird    3.845 0.33188483 -1.1029673\nMerc 450SE          4.070 0.27888986 -1.2769384\nCadillac Fleetwood  5.250 0.04711454 -3.0551736\nChrysler Imperial   5.345 0.03832721 -3.2615952\nLincoln Continental 5.424 0.03205090 -3.4404301\n\nsum(mtcars$log_lik) # any differences here are just rounding error\n\n[1] -44.20914\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause the normal is a continuous distribution, these likelihood values are probability densities. So each value is a likelihood contribution to the overall likelihood.",
    "crumbs": [
      "Section A Recap"
    ]
  },
  {
    "objectID": "chapter-06.html",
    "href": "chapter-06.html",
    "title": "Chapter 6",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Chapter 6"
    ]
  },
  {
    "objectID": "chapter-06.html#overview",
    "href": "chapter-06.html#overview",
    "title": "Chapter 6",
    "section": "",
    "text": "Goals\nThis is our first step into a larger world (of regression analysis). Here, instead of comparing zero-parameter models (MC) to one-parameter models (MA), we’re going to compare one-parameter models (MC) to two-parameter models (MA). The basic question is whether we want to make the same prediction for every observation or different predictions for different observations, condition on their value of some predictor.\nAs before, we’ll augment the book by using simulations to understand null distributions, statistical inference, and power.\n\n\nSet up\nHere are the packages we’re going to need. We’ll also set our {tinyplot} theme.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(stringr)\nlibrary(modelsummary)\nlibrary(tinyplot)\n\ntinytheme(\"ipsum\",\n          family = \"Roboto Condensed\",\n          palette.qualitative = \"Tableau 10\",\n          palette.sequential = \"agSunset\")\n\n\n\nAdditional considerations\nBefore moving onto the chapter proper, I want to say a something about the t-test. The book really doesn’t properly talk about this until Chapter 9, but we’ve been skirting around it and talking about the t distribution already so I’d like to talk a little about this.\n\nThe t-test as a two-group lm() comparison*\nLet’s look at Southern and non-Southern states in 1977 and compare them on income per capita.\n\nstate.x77_with_names &lt;- as_tibble(state.x77,\n                                  rownames = \"state\")\n  \nstates &lt;- bind_cols(state.x77_with_names, \n                    region = state.division) |&gt; # both are in base R\n  janitor::clean_names() |&gt;                     # lower case and underscore\n  mutate(south = as.integer(                    # make a south dummy\n    str_detect(as.character(region), \n               \"South\")))\n\nWe can write:\n\\[\\begin{align}\n&\\text{Model C:  } \\text{LIFEEXP}_i = \\beta_0 + \\epsilon_i \\\\\n\n&\\text{Model A:  } \\text{LIFEEXP}_i = \\beta_0 + \\beta_1 (\\text{SOUTH}_i) + \\epsilon_i\n\\end{align}\\]\nEstimate the models.\n\nmc &lt;- lm(life_exp ~ 1, \n         data = states)\nma &lt;- lm(life_exp ~ 1 + south, \n         data = states)\n\nWe can see the output using the {modelsummary} package, using the msummary() alias for short.\n\nmsummary(list(\"MC\" = mc, \n              \"MA\" = ma),\n         fmt = 2,\n         estimate = \"{estimate} ({std.error})\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                MC\n                MA\n              \n        \n        \n        \n                \n                  (Intercept)\n                  70.88 (0.19)\n                  71.43 (0.19)\n                \n                \n                  south\n                  \n                  -1.72 (0.33)\n                \n                \n                  Num.Obs.\n                  50\n                  50\n                \n                \n                  F\n                  \n                  27.739\n                \n                \n                  R2\n                  0.000\n                  0.366\n                \n        \n      \n    \n\n\n\nThis “regression with a single dummy predictor” is exactly the same as a t-test (that assumes that the two groups have equal within-group variances). Again, we’ll talk more about that later.\n\n\nTwo approaches to coding a predictor\nThe coding of south above is called dummy coding or reference coding and is by far the most common in sociology. But sometimes (including in the book) you will see effect coding (also called deviation or contrast coding) that codes the predictor (-1, 1) instead of (0, 1).\n\nstates &lt;- states |&gt; \n  mutate(south_eff = if_else(south == 1, 1, -1))\n\nma_effect &lt;- lm(life_exp ~ south_eff, data = states)\n\n\n\nShow code\nmsummary(list(\"MA (dummy)\"  = ma, \n              \"MA (effect)\" = ma_effect),\n         fmt = 2,\n         estimate = \"{estimate} ({std.error})\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                MA (dummy)\n                MA (effect)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  71.43 (0.19)\n                  70.57 (0.16)\n                \n                \n                  south\n                  -1.72 (0.33)\n                  \n                \n                \n                  south_eff\n                  \n                  -0.86 (0.16)\n                \n                \n                  Num.Obs.\n                  50\n                  50\n                \n                \n                  F\n                  27.739\n                  27.739\n                \n                \n                  R2\n                  0.366\n                  0.366\n                \n        \n      \n    \n\n\n\nWith effect coding, the intercept is the grand mean, or the overall mean you’d expect if both groups were the same size. And \\(b_1\\) is half the difference between the two groups.",
    "crumbs": [
      "Chapter 6"
    ]
  },
  {
    "objectID": "chapter-06.html#defining-a-linear-two-parameter-model",
    "href": "chapter-06.html#defining-a-linear-two-parameter-model",
    "title": "Chapter 6",
    "section": "Defining a linear two-parameter model",
    "text": "Defining a linear two-parameter model\nAs in the book, we’ll look at regression first through the lens of two continuous variables.\n\n\nShow code\nplt(life_exp ~ hs_grad,\n    data = states,\n    ylim = c(67, 74),\n    main = \"Life expectancy and education\",\n    sub = \"US states, 1977\",\n    xlab = \"% high school graduates\",\n    ylab = \"Life expectancy at birth\")\n\n\n\n\n\n\n\n\n\nWe can add a regression line to this using plt_add().\n\n\nShow code\nplt(life_exp ~ hs_grad,\n    data = states,\n    ylim = c(67, 74),\n    main = \"Life expectancy and education\",\n    sub = \"US states, 1977\",\n    xlab = \"% high school graduates\",\n    ylab = \"Life expectancy at birth\")\nplt_add(type = \"lm\",\n        level = .99)\n\n\n\n\n\n\n\n\n\nContrast that to assuming the same prediction for each state.\n\n\nShow code\nstates$mean_lex &lt;- mean(states$life_exp)\nwidth &lt;- sd(states$life_exp / sqrt(nrow(states))) * qt(.995, 49) # 99%\n\nplt(mean_lex ~ hs_grad,\n    data = states,\n    ymax = mean_lex + width,\n    ymin = mean_lex - width,\n    ylim = c(67, 74),\n    type = \"ribbon\",\n    main = \"Life expectancy and education\",\n    sub = \"US states, 1977\",\n    xlab = \"% high school graduates\",\n    ylab = \"Life expectancy at birth\")\nplt_add(life_exp ~ hs_grad,\n    data = states,\n    type = \"p\")",
    "crumbs": [
      "Chapter 6"
    ]
  },
  {
    "objectID": "chapter-06.html#estimating-a-linear-two-parameter-model",
    "href": "chapter-06.html#estimating-a-linear-two-parameter-model",
    "title": "Chapter 6",
    "section": "Estimating a linear two-parameter model",
    "text": "Estimating a linear two-parameter model\nEstimate the models.\n\nmc &lt;- lm(life_exp ~ 1, data = states)\nma &lt;- lm(life_exp ~ 1 + hs_grad, data = states)\n\n\n\nShow code\nmsummary(list(\"MC\" = mc, \n              \"MA\" = ma),\n         fmt = 3,\n         estimate = \"{estimate} ({std.error})\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                MC\n                MA\n              \n        \n        \n        \n                \n                  (Intercept)\n                  70.879 (0.190)\n                  65.740 (1.047)\n                \n                \n                  hs_grad\n                  \n                  0.097 (0.020)\n                \n                \n                  Num.Obs.\n                  50\n                  50\n                \n                \n                  F\n                  \n                  24.615\n                \n                \n                  R2\n                  0.000\n                  0.339",
    "crumbs": [
      "Chapter 6"
    ]
  },
  {
    "objectID": "chapter-06.html#an-alternative-specification-or-five",
    "href": "chapter-06.html#an-alternative-specification-or-five",
    "title": "Chapter 6",
    "section": "An alternative specification (or five!)",
    "text": "An alternative specification (or five!)\n\nCentering\n\nstates &lt;- states |&gt; \n  mutate(c_hs_grad = hs_grad - mean(hs_grad))\n\n\n\nShow code\nplt(life_exp ~ c_hs_grad,\n    data = states,\n    ylim = c(67, 74),\n    main = \"Life expectancy and education\",\n    sub = \"US states, 1977\",\n    xlab = \"% high school graduates (centered)\",\n    ylab = \"Life expectancy at birth\")\nplt_add(type = \"lm\",\n        level = .99)\n\n\n\n\n\n\n\n\n\n\n\nShow code\nfit_cent &lt;- lm(life_exp ~ c_hs_grad,\n               data = states)\n\nmsummary(list(\"Centered\" = fit_cent),\n         fmt = 3,\n         estimate = \"{estimate} ({std.error})\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Centered\n              \n        \n        \n        \n                \n                  (Intercept)\n                  70.879 (0.156)\n                \n                \n                  c_hs_grad\n                  0.097 (0.020)\n                \n                \n                  Num.Obs.\n                  50\n                \n                \n                  F\n                  24.615\n                \n                \n                  R2\n                  0.339\n                \n        \n      \n    \n\n\n\nThe intercept, \\(b_0\\), is now the expected value of the outcome when hs_grad is at its mean. But the slope, \\(b_1\\), is the same.\n\n\nNormalization*\nSee Cohen et al., “The Problem of Units and the Circumstance for POMP”.\nCreate normalized (or “POMP”) predictor and visualize.\n\nstates &lt;- states |&gt; \n  mutate(hs_grad01 = (hs_grad - min(hs_grad)) /     # subtract min\n                     (max(hs_grad) - min(hs_grad)), # div by range\n         hs_grad01 = scales::rescale(hs_grad))      # an easier way\n\n\n\nShow code\nplt(life_exp ~ hs_grad01,\n    data = states,\n    ylim = c(67, 74),\n    main = \"Life expectancy and education\",\n    sub = \"US states, 1977\",\n    xlab = \"% high school graduates (normalized)\",\n    ylab = \"Life expectancy at birth\")\nplt_add(type = \"lm\",\n        level = .99)\n\n\n\n\n\n\n\n\n\n\n\nShow code\nfit_pomp &lt;- lm(life_exp ~ hs_grad01,\n               data = states)\n\nmsummary(list(\"Norm.\" = fit_pomp),\n         fmt = 3,\n         estimate = \"{estimate} ({std.error})\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Norm.\n              \n        \n        \n        \n                \n                  (Intercept)\n                  69.397 (0.337)\n                \n                \n                  hs_grad01\n                  2.855 (0.575)\n                \n                \n                  Num.Obs.\n                  50\n                \n                \n                  F\n                  24.615\n                \n                \n                  R2\n                  0.339\n                \n        \n      \n    \n\n\n\nNow \\(b_0\\) is the predicted value when X is at its sample minimum and \\(b_1\\) is how much the prediction would change when X is set to its sample maximum. So here moving from the least- to most-educated state, the model would expect life expectancy to be about 2.9 years higher.\n\n\nX-standardization*\nSometimes when Y is in naturally interpretable units (like years) and X is a bit more abstract, we can use x-standardization. This means converting X into a z-score before using it as predictor.\n\\[X^* = \\frac{X_i-\\bar{X}}{s_X}\\]\n\nstates &lt;- states |&gt;\n  mutate(z_hs_grad = (hs_grad - mean(hs_grad)) / sd(hs_grad))\n\n\n\nShow code\nplt(life_exp ~ z_hs_grad,\n    data = states,\n    ylim = c(67, 74),\n    main = \"Life expectancy and education\",\n    sub = \"US states, 1977\",\n    xlab = \"% high school graduates (z-standardized)\",\n    ylab = \"Life expectancy at birth\")\nplt_add(type = \"lm\",\n        level = .99)\n\n\n\n\n\n\n\n\n\n\n\nShow code\nfit_stdx &lt;- lm(life_exp ~ z_hs_grad,\n               data = states)\n\nmsummary(list(\"X std.\" = fit_stdx),\n         fmt = 3,\n         estimate = \"{estimate} ({std.error})\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                X std.\n              \n        \n        \n        \n                \n                  (Intercept)\n                  70.879 (0.156)\n                \n                \n                  z_hs_grad\n                  0.782 (0.158)\n                \n                \n                  Num.Obs.\n                  50\n                \n                \n                  F\n                  24.615\n                \n                \n                  R2\n                  0.339\n                \n        \n      \n    \n\n\n\nThis now means that a plus one standard deviation difference in the % of high school graduates predicts .78 additional years of life expectancy.\n\n\nFull standardization*\nBoth X and Y can also be standardized in this way.\n\n\nShow code\nstates &lt;- states |&gt; \n  mutate(z_life_exp = (life_exp - (mean(life_exp))) / sd(life_exp))\n\n\n\n\nShow code\nplt(z_life_exp ~ z_hs_grad,\n    data = states,\n    main = \"Life expectancy and education\",\n    sub = \"US states, 1977\",\n    xlab = \"% high school graduates (z-standardized)\",\n    ylab = \"Life expectancy at birth (z-standardized)\")\nplt_add(type = \"lm\",\n        level = .99)\n\n\n\n\n\n\n\n\n\n\n\nShow code\nfit_std &lt;- lm(z_life_exp ~ z_hs_grad,\n              data = states)\n\nmsummary(list(\"XY std.\" = fit_std),\n         fmt = 3,\n         estimate = \"{estimate} ({std.error})\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                XY std.\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -0.000 (0.116)\n                \n                \n                  z_hs_grad\n                  0.582 (0.117)\n                \n                \n                  Num.Obs.\n                  50\n                \n                \n                  F\n                  24.615\n                \n                \n                  R2\n                  0.339\n                \n        \n      \n    \n\n\n\nHere the intercept will always be zero, since simple regression always expects the outcome to be at its average when the predictor is at its average. (And here, both averages are zero.)\nThe slope here means that “in a state that is one SD more educated, we expect life expectancy to be .582 SD higher.”\nA fully standardized simple regression coefficient is exactly the same as Pearson’s r or the correlation coefficient. And this value squared is exactly R2 (hence the name!), or PRE.1\n1 This to me is the clearest take on the meaning of the correlation coefficient.\n\nComparing all versions*\nLet’s compare all versions.\n\n\nShow code\nmsummary(list(\"Original\" = ma,\n              \"Centered\" = fit_cent,\n              \"Norm.\" = fit_pomp,\n              \"X-std.\" = fit_stdx,\n              \"XY-std.\" = fit_std),\n         fmt = 3,\n         estimate = \"{estimate}\",\n         statistic = NULL,\n         gof_map = c(\"nobs\", \"F\", \"r.squared\")) \n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Original\n                Centered\n                Norm.\n                X-std.\n                XY-std.\n              \n        \n        \n        \n                \n                  (Intercept)\n                  65.740\n                  70.879\n                  69.397\n                  70.879\n                  -0.000\n                \n                \n                  hs_grad\n                  0.097\n                  \n                  \n                  \n                  \n                \n                \n                  c_hs_grad\n                  \n                  0.097\n                  \n                  \n                  \n                \n                \n                  hs_grad01\n                  \n                  \n                  2.855\n                  \n                  \n                \n                \n                  z_hs_grad\n                  \n                  \n                  \n                  0.782\n                  0.582\n                \n                \n                  Num.Obs.\n                  50\n                  50\n                  50\n                  50\n                  50\n                \n                \n                  F\n                  24.615\n                  24.615\n                  24.615\n                  24.615\n                  24.615\n                \n                \n                  R2\n                  0.339\n                  0.339\n                  0.339\n                  0.339\n                  0.339\n                \n        \n      \n    \n\n\n\nAs you can see, the fit (F, PRE) stays the same but the interpretation of the coefficients changes. That difference in coefficients is for human interpretability, so use whichever makes the most sense to you and your likely readers.",
    "crumbs": [
      "Chapter 6"
    ]
  },
  {
    "objectID": "chapter-06.html#statistical-inference-in-two-parameter-models",
    "href": "chapter-06.html#statistical-inference-in-two-parameter-models",
    "title": "Chapter 6",
    "section": "Statistical inference in two-parameter models",
    "text": "Statistical inference in two-parameter models\n\nTesting the null\nWe’re going to do this with simulations. Specifically, we’re going to implement a permutation test. That means keeping all the individual X values and all the individual Y values as they are in the real data. But we pair them up randomly, which makes the columns independent of each other.\nLet’s make a function to randomly pair x and y values and then estimate a regression line.\n\n# NB: we could make this faster but I want to make it clear...\n\n# this will make the code less messy\nx &lt;- states$hs_grad\ny &lt;- states$life_exp\n\nget_coefs &lt;- function(x, y) {\n  d &lt;- tibble(\n    y = sample(y, length(y)),\n    x = sample(x, length(x))\n  )\n\n  fit &lt;- lm(y ~ x, data = d)\n\n  tibble(\n    b0 = unname(coef(fit)[1]),\n    b1 = unname(coef(fit)[2])\n  )\n}\n\nMake skeleton and get regression lines from each run of get_coefs().\n\nmynulls &lt;- tibble(id = 1:1000) |&gt;\n  rowwise() |&gt;\n  mutate(coefs = list(get_coefs(x, y))) |&gt;\n  tidyr::unnest_wider(coefs) |&gt;\n  ungroup()\n\nConvert the estimates to predictions for plotting.\n\nx_max &lt;- max(states$hs_grad)\nx_min &lt;- min(states$hs_grad)\n\nmysticks &lt;- mynulls |&gt; \n  transmute(\n    id = factor(id),\n    x1 = x_min,\n    x2 = x_max,\n    y1 = b0 + b1 * x1,\n    y2 = b0 + b1 * x2\n  ) |&gt; \n  pivot_longer(\n    cols = c(x1, x2, y1, y2),\n    names_to = c(\".value\", \"pt\"),\n    names_pattern = \"([xy])(\\\\d)\"\n  )\n\nNow plot the “bundle of null sticks” (as I like to think of it).\n\nplt(y ~ x,\n    data = mysticks[1:200, ], # don't want too many\n    type = \"l\",\n    alpha = .4,\n    ylim = c(67, 74),\n    xlim = c(x_min, x_max),\n    main = \"Null regression lines and estimated line\",\n    sub = \"Nulls from 100 permuations\",\n    xlab = \"% HS grad\",\n    ylab = \"Life expectancy\")\nplt_add(type = type_abline(a = coef(ma)[1], \n                           b = coef(ma)[2]),\n        lw = 3,\n        col = \"#F28E2B\")\n\n\n\n\n\n\n\n\nWe see that the estimated regression line is quite a bit different from the lines built in a world where the null hypothesis is true (i.e., X and Y are independent).\nWe can make this a little easier to see by plotting the \\(b_1\\) null estimates with the estimated value of \\(b_1\\).\n\n\nShow code\nest_b1 &lt;- ma$coefficients[2] \n\nplt(~ b1,\n    data = mynulls,\n    type = \"hist\",\n    main = \"Observed and null estimates of b1\",\n    xlim = c(min(mynulls$b1) * 1.05 , est_b1 * 1.05))\nplt_add(type = type_vline(v = est_b1),\n        col = \"#F28E2B\",\n        lw = 2)\n\n\n\n\n\n\n\n\n\nThe observed slope is actually larger than all 1000 null slopes.\n\n\nConfidence intervals for \\(\\beta_1\\)\nThis is basically the same as before: the estimate plus and minus a certain number of standard errors depending on the desired interval.\nConsider this result again:\n\ntidy(ma, conf.int = TRUE, conf.level = .99)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  65.7       1.05       62.8  9.92e-48  62.9       68.5  \n2 hs_grad       0.0968    0.0195      4.96 9.20e- 6   0.0445     0.149\n\n\nConsider all the ways that these quantities relate to each other. Make sure you understand the connection between t, F, \\(b_1\\), the standard error, R2, the confidence interval, etc. Which ones can you make out of which other ones?\nWe can, of course, also construct a confidence interval using simulations rather than formulas. We can use bootstrapping2 for this. You probably get the drill by now.\n2 Reminder: bootstrapping means repeatedly sampling from the real data with replacement, which allows us to incorporate sampling variability into our estimates.\nset.seed(0213)\n\n# function to do what I want once\nget_boot_slope &lt;- function() {\n  \n  fit &lt;- lm(life_exp ~ hs_grad,\n            data = slice_sample(states,            # here's the BS sample\n                                n = nrow(states),\n                                replace = TRUE))\n  \n  b1 &lt;- coef(fit)[2]\n  \n  return(b1)\n  \n}\n\n# skeleton and apply\nmysims &lt;- tibble(\n  id = 1:4000) |&gt; \n  rowwise() |&gt; \n  mutate(b1 = get_boot_slope())\n\nHere are the bootstrap estimates.\n\n\nShow code\nplt(~ b1,\n    data = mysims,\n    type = type_hist(),\n    main = \"Bootstrap estimates of b1\")\n\n\n\n\n\n\n\n\n\nWe can calculate the 99% interval two ways. The percentile bootstrap will just take the 0.5% and 99.5% values of the simulated slopes:\n\nquantile(mysims$b1, c(.005, .995))\n\n      0.5%      99.5% \n0.03292772 0.15565012 \n\n\nThe normal or empirical bootstrap will use the SD of these simulations to construct the standard error. Then we can use that number in conjunction with the original regression estimate of \\(b_1\\) to create the interval.\n\nse &lt;- sd(mysims$b1)\nalpha &lt;- .01\nt_width &lt;- qt(1 - (alpha / 2), 48)\nest_b1 &lt;- coef(ma)[2]\n\nc(est_b1 - t_width * se,\n  est_b1 + t_width * se)\n\n   hs_grad    hs_grad \n0.03260387 0.16092430 \n\n\nThe SE we get from bootstrapping is 0.024. This is close to, but larger than, the theory-based estimate we got from lm(). This is for reasons that we’re not going to go into here.\n\n\nPower analysis\nUsing fully standardized variables allows us to think about power pretty easily using pwr::pwr.r.test()\n\npwr::pwr.r.test(r = .1, \n                sig.level = .01, \n                power = .9, \n                alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 1480.418\n              r = 0.1\n      sig.level = 0.01\n          power = 0.9\n    alternative = two.sided\n\n\nRather than doing these one by one, we can see the general pattern.\n\n\nShow code\n# function\nget_n &lt;- function(r, power = .8, alpha = .01) {\n  \n  tmp &lt;- pwr::pwr.r.test(r = r, \n                         sig.level = alpha, \n                         power = power, \n                         alternative = \"two.sided\")\n  \n  return(tmp$n)\n  \n}\n\n# skeleton and apply\nn_needed &lt;- expand_grid(\n  r = seq(from = .1, to = .5, by = .01),\n  power = c(.80, .99),\n  alpha = .01) |&gt;\n  \n  rowwise() |&gt; \n  mutate(n = get_n(r = r, power = power))\n\n# plot\nplt(n ~ r | factor(power),\n    data = n_needed,\n    type = \"l\",\n    lw = 3,\n    ylim = c(0, 2500),\n    main = \"Sample size needed to detect r\",\n    sub = expression(\"two-sided\" ~ alpha ~ \"= .01\"),\n    xlab = \"Pearson's r\",\n    ylab = \"N\",\n    legend = legend(\"topright!\", title = \"Power\"))\n\n\n\n\n\n\n\n\n\nWhat we see is that it’s very, very expensive to try and detect small effects! (And of course we could do this via simulation but I think you have the picture by now!)\n\n\nTwo-parameter model comparison\nThe book has a brief discussion of situations where we might jointly test two null hypotheses at the same time. Let’s look at another example of that using our GSS TV data.\n\n\nShow code\ngss_panel10_long &lt;- readr::read_rds(here::here(\"data\", \"gss_panel10_long.rds\"))\n\nd &lt;- gss_panel10_long |&gt; \n  select(firstid, wave, y = tvhours) |&gt; \n  filter(wave %in% c(1, 3)) |&gt; \n  drop_na() |&gt;\n  mutate(y = as.numeric(y)) |&gt; \n  mutate(num_obs = n(), .by = firstid) |&gt; \n  filter(num_obs == 2) |&gt; \n  select(-num_obs) |&gt; \n  pivot_wider(id_cols = firstid,\n              names_prefix = \"y\",\n              names_from = wave,\n              values_from = y)\n\n\nHere are the first few rows of the data.\n\n\nShow code\nhead(d)\n\n\n# A tibble: 6 × 3\n  firstid    y1    y3\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 0001        2     3\n2 0002        1     2\n3 0003        6     6\n4 0006        5     4\n5 0010        2     2\n6 0011        5     3\n\n\nWe can think about a model where \\(TV3\\) is a function of \\(TV1\\). Like with the “father-son heights” example from the book, we can stipulate two models: a null model where 2014 TV hours is exactly the same as 2010 TV hours and another where we estimate it from the data. The null here is no one has changed at all.3\n3 This is different from saying that the average change is zero, which is what we tested in the last chapter. The null hypothesis here is basically equivalent to asserting that the Wave 1 and Wave 3 data are identical, which is not something we’d ever do. That said, this is exactly the same in spirit as the “fathers and sons are the same height” example from the book!\\[\\begin{align}\n  \n  \\text{Model C: } TV3_i &= 0 + 1(TV1_i) + \\epsilon_i \\\\\n  \\text{Model A: } TV3_i &= \\beta_0 + \\beta_1(TV1_i) + \\epsilon_i \\\\\n\n\\end{align}\\]\nSo we have:\n\\[\\begin{align}\n\n  \\mathcal{H}_0: &\\beta_0 = 0; \\beta_1=1 \\\\\n  \\mathcal{H}_1: &\\beta_0 \\neq 0; \\beta_1 \\neq 1\n\n\\end{align}\\]\nWe can kind of trick R to estimate the first model (using offset() to constrain \\(\\beta_1 = 1\\)) and use it in the normal way to get the second.\n\nmc &lt;- lm(y3 ~ 0 + offset(y1), data = d)\nma &lt;- lm(y3 ~ 1 + y1, data = d)\n\nanova(mc, ma)\n\nAnalysis of Variance Table\n\nModel 1: y3 ~ 0 + offset(y1)\nModel 2: y3 ~ 1 + y1\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    900 5045.0                                  \n2    898 3720.2  2    1324.8 159.89 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs is clear from the output, we’re doing an F-test with 2 numerator df and 898 denominator df. We can clearly reject the null that the responses are identical!",
    "crumbs": [
      "Chapter 6"
    ]
  }
]